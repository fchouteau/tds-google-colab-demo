{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a basic CNN with Ignite on Google Colaboratory\n",
    "Toulouse Data Science 18/06/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of custom dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Houston we have GPU\n",
    "!nvcc -V\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T19:38:19.646346Z",
     "start_time": "2019-06-16T19:38:19.109520Z"
    }
   },
   "outputs": [],
   "source": [
    "# List existing dependencies\n",
    "!pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation of custom dependencies\n",
    "!pip3 install tqdm pytorch-ignite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data from google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the mount as storage option but there are many\n",
    "# See https://colab.research.google.com/notebooks/io.ipynb#scrollTo=RWSJpsyKqHjH for reference\n",
    "try:\n",
    "    import subprocess\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we are on google colab we run this to get data\n",
    "!cp /content/gdrive/My\\ Drive/eurosat.tar.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# else we run this\n",
    "!gsutil -m cp -r gs://fchouteau-storage/eurosat.tar.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -zxf eurosat.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls eurosat/\n",
    "!ls eurosat/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various imports\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "from itertools import cycle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import ignite.engine\n",
    "import ignite.metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, ToTensor, RandomHorizontalFlip, RandomVerticalFlip\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2019)\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "MAX_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Datasets and dataloaders definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "class EurosatDataset(Dataset):\n",
    "    def __init__(self, images_files, shuffle=True):\n",
    "        self.images = images_files\n",
    "        self.classes = list(set([f.split(\"/\")[-2] for f in self.images]))\n",
    "        self.classes = sorted(self.classes)\n",
    "\n",
    "        if shuffle:\n",
    "            random.shuffle(self.images)\n",
    "\n",
    "    @classmethod\n",
    "    def from_fold(cls, data_dir, fold, shuffle=True):\n",
    "        images = glob.glob(os.path.join(data_dir, fold, \"**\", \"*.jpg\"))\n",
    "        return cls(images, shuffle=shuffle)\n",
    "\n",
    "    def split(self, n=0.75):\n",
    "        n = int(n * len(self))\n",
    "        return EurosatDataset(self.images[:n], shuffle=True), EurosatDataset(self.images[n:], shuffle=True)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = self.images[item]\n",
    "        img = Image.open(img)\n",
    "        cls = self.classes.index(self.images[item].split(\"/\")[-2])\n",
    "\n",
    "        return img, cls\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "class TransformedDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None, target_transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        x, y = self.dataset[item]\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        if self.target_transform is not None:\n",
    "            y = self.target_transform(y)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T18:36:39.266708Z",
     "start_time": "2019-06-16T18:36:39.118817Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Get data loaders functions\n",
    "\n",
    "\n",
    "def get_datasets(root_dir):\n",
    "\n",
    "    train_data_transform = Compose([RandomHorizontalFlip(), RandomVerticalFlip(), ToTensor()])\n",
    "    val_data_transform = Compose([ToTensor()])\n",
    "\n",
    "    train_dataset = EurosatDataset.from_fold(data_dir=root_dir, fold=\"train\")\n",
    "    train_dataset, val_dataset = train_dataset.split(n=0.7)\n",
    "    test_dataset = EurosatDataset.from_fold(data_dir=root_dir, fold=\"test\")\n",
    "\n",
    "    train_dataset = TransformedDataset(dataset=train_dataset, transform=train_data_transform)\n",
    "    val_dataset = TransformedDataset(dataset=val_dataset, transform=val_data_transform)\n",
    "    test_dataset = TransformedDataset(dataset=test_dataset, transform=val_data_transform)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "def get_data_loaders(train_dataset, val_dataset):\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "    val_loader = DataLoader(dataset=val_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model to be trained\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU())\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU())\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(64, 96, kernel_size=3, padding=1), nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(nn.Linear(4 * 4 * 96, 32), nn.ReLU())\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 64x64x3\n",
    "        x = F.max_pool2d(self.conv1(x), 2)  # 32x32x32\n",
    "        x = F.max_pool2d(self.conv2(x), 2)  # 16x16x64\n",
    "        x = F.max_pool2d(self.conv3(x), 2)  # 8x8x64\n",
    "        x = F.max_pool2d(self.conv4(x), 2)  # 4x4x96\n",
    "        x = F.dropout2d(x, training=self.training)\n",
    "        x = x.view(-1, 4 * 4 * 96)\n",
    "        x = self.fc1(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datasets and models\n",
    "train_dataset, val_dataset, test_dataset = get_datasets(root_dir=\"./eurosat\")\n",
    "train_loader, val_loader = get_data_loaders(train_dataset, val_dataset)\n",
    "model = SimpleCNN()\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize training resources\n",
    "optimizer = SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, nesterov=True)\n",
    "trainer = ignite.engine.create_supervised_trainer(model, optimizer, F.nll_loss, device=device)\n",
    "evaluator = ignite.engine.create_supervised_evaluator(\n",
    "    model, metrics={\n",
    "        'accuracy': ignite.metrics.Accuracy(),\n",
    "        'nll': ignite.metrics.Loss(F.nll_loss)\n",
    "    }, device=device)\n",
    "\n",
    "desc = \"ITERATION - loss: {:.2f}\"\n",
    "pbar = tqdm(initial=0, leave=False, total=len(train_loader), desc=desc.format(0))\n",
    "\n",
    "\n",
    "@trainer.on(ignite.engine.Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    iter = (engine.state.iteration - 1) % len(train_loader) + 1\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        pbar.desc = desc.format(engine.state.output)\n",
    "        pbar.update(100)\n",
    "\n",
    "\n",
    "@trainer.on(ignite.engine.Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    pbar.refresh()\n",
    "    evaluator.run(train_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    avg_accuracy = metrics['accuracy']\n",
    "    avg_nll = metrics['nll']\n",
    "    tqdm.write(\"Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\".format(\n",
    "        engine.state.epoch, avg_accuracy, avg_nll))\n",
    "\n",
    "\n",
    "@trainer.on(ignite.engine.Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(val_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    avg_accuracy = metrics['accuracy']\n",
    "    avg_nll = metrics['nll']\n",
    "    tqdm.write(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\".format(\n",
    "        engine.state.epoch, avg_accuracy, avg_nll))\n",
    "\n",
    "    pbar.n = pbar.last_print_n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go !\n",
    "trainer.run(train_loader, max_epochs=MAX_EPOCHS)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test dataset (PR curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T18:38:09.751962Z",
     "start_time": "2019-06-16T18:38:09.672176Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T18:38:09.842808Z",
     "start_time": "2019-06-16T18:38:09.754914Z"
    }
   },
   "outputs": [],
   "source": [
    "test_iterator = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y_preds = []\n",
    "all_y_trues = []\n",
    "for x, y_true in test_iterator:\n",
    "    if torch.cuda.is_available():\n",
    "        y_pred = F.softmax(model(x.cuda()), dim=-1).detach().cpu().numpy()\n",
    "        y_true = y_true.cpu().numpy()\n",
    "    else:\n",
    "        y_pred = F.softmax(model(x), dim=-1).detach().numpy()\n",
    "        y_true = y_true.numpy()\n",
    "    y_true = np.eye(10)[y_true]\n",
    "    all_y_trues.extend(y_true)\n",
    "    all_y_preds.extend(y_pred)\n",
    "y_pred = np.asarray(all_y_preds)\n",
    "y_true = np.asarray(all_y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T18:38:18.637793Z",
     "start_time": "2019-06-16T18:38:18.551987Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute precision and recall for each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(10):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_true[:, i], y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(y_true[:, i], y_pred[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T18:38:19.088581Z",
     "start_time": "2019-06-16T18:38:18.639437Z"
    }
   },
   "outputs": [],
   "source": [
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_true.ravel(), y_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(y_true, y_pred, average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pr curve (code taken from sklearn)\n",
    "colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal', 'red', 'yellow', 'purple', 'green'])\n",
    "plt.figure(figsize=(7, 8))\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.2f})' ''.format(average_precision[\"micro\"]))\n",
    "for i, color in zip(range(10), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n",
    "                  ''.format(test_dataset.dataset.classes[i], average_precision[i]))\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -1.0), prop=dict(size=14))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
